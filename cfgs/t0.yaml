# experiment configuration
name: t0
save_dir: './save/'
eval_model: false # "/export/pasand/trans-hyper/save/t0pp-calhouse-finonly/epoch-10.pth"
wandb_auth: 'wandb_auth.yaml'
wandb_sweep_cfg: false # 'cfgs/sweeps/sweeptest.yaml'


# debugging configuration
debug: true
debug_datasets: false
debug_hypernet: false
debug_hyponet: false
debug_trainer: false


# hypernet configuration
hypernet:
  name: t0
  model: bigscience/T0pp


# hyponet configuration
hyponet:
  model: mlp
  in_dim: 103
  out_dim: 2
  hidden_dim: 10
  depth: 5


# tokenizer configuration
tokenizer:
  name: t0_tokenizer
  model: bigscience/T0pp
  truncation: True
  padding: max_length
  max_length: 128


# trainer configuration
trainer:
  name: t0_trainer
  batch_size: 1
  n_workers: 1
  max_epoch: 40
  eval_epoch: 1
  vis_epoch: 25
  save_epoch: 4
  optimizer:
    name: adam
    args:
      lr: 1.e-5
    
# dataset configuration
datasets:
  data_root: ./data/
  n_shots: 4
  n_queries: 4
  train_ratio: None
  test_ratio: None
  train_size: 40
  val_size: 40
  test_size: 1000

  balanced:
    train: True
    val: True
    test: False

  list_combine_train: # list of datasets to combine
    - adult_income
    - loan_default
        
  list_combine_val: # list of datasets to combine for validation
    - credit_risk
  
  list_combine_test: # list of datasets to combine for testing
    - credit_risk
  
  tabllm:
    raw_data_path: "tabllm/data/datasets"
    txt_data_path: "tabllm/data/datasets_serialized"
    template_dir: "tabllm/templates"


