{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5313ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "from datahandles import preprocess_numeric\n",
    "from utils import Config\n",
    "\n",
    "from tabllm import load_dataset, load_and_preprocess_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84517692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       white_piece0_strength  white_piece0_file  white_piece0_rank  \\\n",
      "0                        0.0                1.0                8.0   \n",
      "1                        0.0                2.0                8.0   \n",
      "2                        0.0                4.0                8.0   \n",
      "3                        0.0                5.0                8.0   \n",
      "4                        0.0                6.0                8.0   \n",
      "...                      ...                ...                ...   \n",
      "44814                    7.0                6.0                1.0   \n",
      "44815                    7.0                0.0                0.0   \n",
      "44816                    7.0                1.0                0.0   \n",
      "44817                    7.0                2.0                0.0   \n",
      "44818                    7.0                4.0                0.0   \n",
      "\n",
      "       black_piece0_strength  black_piece0_file  black_piece0_rank  label  \n",
      "0                        0.0                0.0                8.0   True  \n",
      "1                        0.0                0.0                8.0   True  \n",
      "2                        0.0                0.0                8.0   True  \n",
      "3                        0.0                0.0                8.0   True  \n",
      "4                        0.0                0.0                8.0   True  \n",
      "...                      ...                ...                ...    ...  \n",
      "44814                    7.0                6.0                0.0   True  \n",
      "44815                    7.0                6.0                0.0  False  \n",
      "44816                    7.0                6.0                0.0  False  \n",
      "44817                    7.0                6.0                0.0  False  \n",
      "44818                    7.0                6.0                0.0  False  \n",
      "\n",
      "[44819 rows x 7 columns]\n",
      "Index(['white_piece0_strength', 'white_piece0_file', 'white_piece0_rank',\n",
      "       'black_piece0_strength', 'black_piece0_file', 'black_piece0_rank',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ds_name = 'jungle'\n",
    "ds_raw = load_dataset(dataset_name=ds_name, data_dir=Path(f'./tabllm/data/datasets/{ds_name}'))\n",
    "print(ds_raw)\n",
    "print(ds_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9191ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['white_piece0_rank', 'white_piece0_file', 'black_piece0_strength', 'black_piece0_rank', 'white_piece0_strength', 'black_piece0_file']\n"
     ]
    }
   ],
   "source": [
    "all_cols = ['white_piece0_strength', 'white_piece0_file', 'white_piece0_rank',\n",
    "       'black_piece0_strength', 'black_piece0_file', 'black_piece0_rank']\n",
    "categorical = []\n",
    "numerical = list(set(all_cols) - set(categorical) - set(['label']))\n",
    "\n",
    "print(categorical)\n",
    "print(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f2030f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "False    11687\n",
      "True     11687\n",
      "Name: count, dtype: int64\n",
      "Max const. predictor acc: 50.0\n"
     ]
    }
   ],
   "source": [
    "ds_name = 'income'\n",
    "ds_raw = load_and_preprocess_dataset(dataset_name=ds_name, data_dir=Path(f'./tabllm/data/datasets/{ds_name}'))\n",
    "# print(ds_raw)\n",
    "ds_txt = load_from_disk(f\"tabllm/data/datasets_serialized/{ds_name}\")\n",
    "ds_txt = pd.DataFrame(ds_txt) # type: ignore\n",
    "val_counts = ds_raw['label'].value_counts()\n",
    "print(val_counts)\n",
    "print('Max const. predictor acc:', max(val_counts)/sum(val_counts) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a64513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age         workclass     education      marital_status  \\\n",
      "16465   39  Self-emp-not-inc          11th  Married-civ-spouse   \n",
      "5625    54  Self-emp-not-inc     Bachelors  Married-civ-spouse   \n",
      "30273   32           Private       HS-grad  Married-civ-spouse   \n",
      "3136    45  Self-emp-not-inc  Some-college       Never-married   \n",
      "4521    60           Private          10th  Married-civ-spouse   \n",
      "...    ...               ...           ...                 ...   \n",
      "32511   25         Local-gov     Bachelors       Never-married   \n",
      "5192    32           Private     Bachelors  Married-civ-spouse   \n",
      "12172   27           Private     Bachelors       Never-married   \n",
      "235     59         State-gov       HS-grad  Married-civ-spouse   \n",
      "29733   33           Private     Bachelors  Married-civ-spouse   \n",
      "\n",
      "              occupation   relationship                race     sex  \\\n",
      "16465   Transport-moving        Husband               White    Male   \n",
      "5625     Exec-managerial        Husband               White    Male   \n",
      "30273              Sales        Husband               White    Male   \n",
      "3136     Farming-fishing  Not-in-family               White    Male   \n",
      "4521   Handlers-cleaners        Husband               Black    Male   \n",
      "...                  ...            ...                 ...     ...   \n",
      "32511       Adm-clerical      Own-child               Black  Female   \n",
      "5192     Exec-managerial        Husband               White    Male   \n",
      "12172  Machine-op-inspct  Not-in-family  Asian-Pac-Islander    Male   \n",
      "235        Other-service        Husband               White    Male   \n",
      "29733       Adm-clerical        Husband               White    Male   \n",
      "\n",
      "       capital_gain  capital_loss  hours_per_week native_country  label  \n",
      "16465             0             0              40  United-States  False  \n",
      "5625              0             0              40  United-States   True  \n",
      "30273             0          1902              50  United-States   True  \n",
      "3136              0             0              50  United-States  False  \n",
      "4521              0             0              40  United-States  False  \n",
      "...             ...           ...             ...            ...    ...  \n",
      "32511             0             0              40  United-States  False  \n",
      "5192          15024             0              45  United-States   True  \n",
      "12172             0             0              40            NaN  False  \n",
      "235               0             0              40  United-States  False  \n",
      "29733             0          1902              45  United-States   True  \n",
      "\n",
      "[26048 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"income\"\n",
    "# ds = load_from_disk(f\"tabllm/data/data_ser/{ds_name}\")\n",
    "ds = load_dataset(dataset_name=ds_name, data_dir=Path(f\"./tabllm/data/datasets/{ds_name}\"))\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5ae0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df75bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    # if \"classifier\" not in name: # Assuming the last layer is named \"classifier\"\n",
    "    #     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3603cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabLLMDataObject initialized with 4 training set(s),1 validation set(s), and 1 test set(s).\n",
      "Maximum number of features across all datasets: 103\n",
      "Hyponet in_dim set to max number of features (=103)\n",
      "[(748, 748), (45211, 45211), (20640, 20640), (48842, 48842)]\n",
      "Example: The median income is 3.0375. The median age is 26. The total rooms is 1912. The total bedrooms is 339. The population is 1002. The households is 311. The latitude is 36.37. The longitude is -119.55.\n",
      "\n",
      "Is this house block valuable? Yes or no? Answer:\n",
      "|||\n",
      "No\n",
      " Example: The median income is 3.4419. The median age is 15. The total rooms is 7626. The total bedrooms is 1570. The population is 3823. The households is 1415. The latitude is 33.87. The longitude is -117.60.\n",
      "\n",
      "Is this house block valuable? Yes or no? Answer:\n",
      "|||\n",
      "No\n",
      " Example: The median income is 4.4762. The median age is 30. The total rooms is 5398. The total bedrooms is 926. The population is 2672. The households is 864. The latitude is 33.94. The longitude is -117.55.\n",
      "\n",
      "Is this house block valuable? Yes or no? Answer:\n",
      "|||\n",
      "No\n",
      "\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "from utils import Config, ConfigObject\n",
    "from datahandles import TabLLMDataObject, CombinedTabLLMTextDataset\n",
    "import yaml\n",
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "import os\n",
    "\n",
    "cfg = Config(\"./cfgs/bert.yaml\")\n",
    "tabllm_do = TabLLMDataObject(cfg, set_hyponet_in_dim=True)\n",
    "test_ds = tabllm_do.data['test']\n",
    "print(test_ds[0]['shots'])\n",
    "\n",
    "print(len(tokenizer.tokenize(test_ds[0]['shots'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transhyper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
