{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5313ec9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_diabetes\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatahandles\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_numeric\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "from datahandles import preprocess_numeric\n",
    "from utils import Config\n",
    "\n",
    "from tabllm import load_dataset, load_and_preprocess_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84517692",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'heart'\n",
    "ds_raw = load_dataset(dataset_name=ds_name, data_dir=Path(f'./tabllm/data/datasets/{ds_name}'))\n",
    "print(ds_raw)\n",
    "print(ds_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety_dict']\n",
    "categorical = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety_dict']\n",
    "numerical = list(set(all_cols) - set(categorical) - set(['label']))\n",
    "\n",
    "print(categorical)\n",
    "print(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2030f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "False    19694\n",
      "True      6354\n",
      "Name: count, dtype: int64\n",
      "Max const. predictor acc: 75.60657248157248\n"
     ]
    }
   ],
   "source": [
    "ds_name = 'income'\n",
    "ds_raw = load_dataset(dataset_name=ds_name, data_dir=Path(f'./tabllm/data/datasets/{ds_name}'))\n",
    "# print(ds_raw)\n",
    "ds_txt = load_from_disk(f\"tabllm/data/datasets_serialized/{ds_name}\")\n",
    "ds_txt = pd.DataFrame(ds_txt) # type: ignore\n",
    "val_counts = ds_raw['label'].value_counts()\n",
    "print(val_counts)\n",
    "print('Max const. predictor acc:', max(val_counts)/sum(val_counts) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a64513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age         workclass     education      marital_status  \\\n",
      "16465   39  Self-emp-not-inc          11th  Married-civ-spouse   \n",
      "5625    54  Self-emp-not-inc     Bachelors  Married-civ-spouse   \n",
      "30273   32           Private       HS-grad  Married-civ-spouse   \n",
      "3136    45  Self-emp-not-inc  Some-college       Never-married   \n",
      "4521    60           Private          10th  Married-civ-spouse   \n",
      "...    ...               ...           ...                 ...   \n",
      "32511   25         Local-gov     Bachelors       Never-married   \n",
      "5192    32           Private     Bachelors  Married-civ-spouse   \n",
      "12172   27           Private     Bachelors       Never-married   \n",
      "235     59         State-gov       HS-grad  Married-civ-spouse   \n",
      "29733   33           Private     Bachelors  Married-civ-spouse   \n",
      "\n",
      "              occupation   relationship                race     sex  \\\n",
      "16465   Transport-moving        Husband               White    Male   \n",
      "5625     Exec-managerial        Husband               White    Male   \n",
      "30273              Sales        Husband               White    Male   \n",
      "3136     Farming-fishing  Not-in-family               White    Male   \n",
      "4521   Handlers-cleaners        Husband               Black    Male   \n",
      "...                  ...            ...                 ...     ...   \n",
      "32511       Adm-clerical      Own-child               Black  Female   \n",
      "5192     Exec-managerial        Husband               White    Male   \n",
      "12172  Machine-op-inspct  Not-in-family  Asian-Pac-Islander    Male   \n",
      "235        Other-service        Husband               White    Male   \n",
      "29733       Adm-clerical        Husband               White    Male   \n",
      "\n",
      "       capital_gain  capital_loss  hours_per_week native_country  label  \n",
      "16465             0             0              40  United-States  False  \n",
      "5625              0             0              40  United-States   True  \n",
      "30273             0          1902              50  United-States   True  \n",
      "3136              0             0              50  United-States  False  \n",
      "4521              0             0              40  United-States  False  \n",
      "...             ...           ...             ...            ...    ...  \n",
      "32511             0             0              40  United-States  False  \n",
      "5192          15024             0              45  United-States   True  \n",
      "12172             0             0              40            NaN  False  \n",
      "235               0             0              40  United-States  False  \n",
      "29733             0          1902              45  United-States   True  \n",
      "\n",
      "[26048 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"income\"\n",
    "# ds = load_from_disk(f\"tabllm/data/data_ser/{ds_name}\")\n",
    "ds = load_dataset(dataset_name=ds_name, data_dir=Path(f\"./tabllm/data/datasets/{ds_name}\"))\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5ae0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df75bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    # if \"classifier\" not in name: # Assuming the last layer is named \"classifier\"\n",
    "    #     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3603cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabLLMDataObject initialized with 4 training set(s),1 validation set(s), and 1 test set(s).\n",
      "Maximum number of features across all datasets: 103\n",
      "Hyponet in_dim set to max number of features (=103)\n",
      "[(48842, 48842), (748, 748), (45211, 45211), (20640, 20640)]\n",
      "Example: The median income is 2.1516. The median age is 52. The total rooms is 1254. The total bedrooms is 469. The population is 895. The households is 456. The latitude is 37.78. The longitude is -122.42. The label is True.\n",
      "\n",
      "  Example: The median income is 2.6458. The median age is 50. The total rooms is 629. The total bedrooms is 188. The population is 742. The households is 196. The latitude is 37.79. The longitude is -122.25. The label is False.\n",
      "\n",
      "  Example: The median income is 5.7051. The median age is 35. The total rooms is 2142. The total bedrooms is 373. The population is 986. The households is 374. The latitude is 34.17. The longitude is -118.07. The label is True.\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from utils import Config, ConfigObject\n",
    "from datahandles import TabLLMDataObject, CombinedTabLLMTextDataset\n",
    "import yaml\n",
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "import os\n",
    "\n",
    "cfg = Config(\"./cfgs/bert.yaml\")\n",
    "tabllm_do = TabLLMDataObject(cfg, set_hyponet_in_dim=True)\n",
    "test_ds = tabllm_do.data['test']\n",
    "print(test_ds[0]['shots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1]\n",
      "Example: The median income is 2.1516. The median age is 52. The total rooms is 1254. The total bedrooms is 469. The population is 895. The households is 456. The latitude is 37.78. The longitude is -122.42. The label is True.\n",
      "\n",
      "  Example: The median income is 2.6458. The median age is 50. The total rooms is 629. The total bedrooms is 188. The population is 742. The households is 196. The latitude is 37.79. The longitude is -122.25. The label is False.\n",
      "\n",
      "  Example: The median income is 5.7051. The median age is 35. The total rooms is 2142. The total bedrooms is 373. The population is 986. The households is 374. The latitude is 34.17. The longitude is -118.07. The label is True.\n",
      "\n",
      " \n",
      "\n",
      "Did the person donate blood? Yes or no? Answer: ||| \n"
     ]
    }
   ],
   "source": [
    "class CustomTagLoader(yaml.SafeLoader):\n",
    "    pass\n",
    "\n",
    "# Add constructors for the specific tags you want to treat as regular mappings.\n",
    "# This tells PyYAML to parse the content under these tags as a standard dictionary.\n",
    "CustomTagLoader.add_constructor(u'!Template', CustomTagLoader.construct_mapping)\n",
    "CustomTagLoader.add_constructor(u'!TemplateMetadata', CustomTagLoader.construct_mapping)\n",
    "\n",
    "\n",
    "def load_and_apply_template(file_path: str, note: str, label: int) -> str:\n",
    "    \"\"\"\n",
    "    Loads a YAML template from a file, applies the given note and label using Jinja2.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the YAML template file.\n",
    "        note (str): The note text to be inserted into the template.\n",
    "        label (int): The index of the answer choice to be selected (0 for 'No', 1 for 'Yes').\n",
    "\n",
    "    Returns:\n",
    "        str: The rendered string from the Jinja template.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the YAML content from the file using the custom loader\n",
    "        with open(file_path, 'r') as f:\n",
    "            template_data = yaml.load(f, Loader=CustomTagLoader)\n",
    "\n",
    "        # Extract the relevant template information\n",
    "        # Assuming the structure is dataset -> templates -> <uuid> -> jinja\n",
    "        # And answer_choices is also under the <uuid>\n",
    "        template_id = list(template_data['templates'].keys())[0] # Get the first (and likely only) template ID\n",
    "        template_config = template_data['templates'][template_id]\n",
    "\n",
    "        jinja_string = template_config['jinja']\n",
    "        answer_choices_raw = template_config['answer_choices']\n",
    "\n",
    "        # Split answer choices into a list\n",
    "        answer_choices = [choice.strip() for choice in answer_choices_raw.split('|||')]\n",
    "\n",
    "        # Set up Jinja2 environment (no specific loader needed as we have the string directly)\n",
    "        env = Environment(\n",
    "            loader=FileSystemLoader(os.path.dirname(file_path) or './'), # Use FileSystemLoader for relative includes, though not strictly needed here\n",
    "            autoescape=select_autoescape(['html', 'xml'])\n",
    "        )\n",
    "\n",
    "        # Load the template from the string\n",
    "        template = env.from_string(jinja_string)\n",
    "\n",
    "        # Render the template with the provided data\n",
    "        rendered_output = template.render(\n",
    "            note=note,\n",
    "            label=label,\n",
    "            answer_choices=answer_choices # Pass the list of answer choices\n",
    "        )\n",
    "\n",
    "        return rendered_output\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {file_path}\"\n",
    "    except yaml.YAMLError as e:\n",
    "        return f\"Error parsing YAML file: {e}\"\n",
    "    except KeyError as e:\n",
    "        return f\"Error: Missing key in template structure: {e}. Please check the YAML format.\"\n",
    "    except IndexError:\n",
    "        return f\"Error: Label {label} is out of bounds for answer choices {answer_choices}. Please provide a valid label index.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "print(test_ds[0]['queries_y'])\n",
    "out = load_and_apply_template(\"./tabllm/templates/templates_blood.yaml\", note=test_ds[0]['shots'], label=test_ds[0]['queries_y'])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e5ec1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 1 (Label 'Yes') ---\n",
      "{'dataset': 'bank', 'templates': {'fc3d4b70-f59e-4fbb-9582-7e118946162c': None}}\n",
      "An unexpected error occurred: 'NoneType' object is not subscriptable\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Example 2 (Label 'No') ---\n",
      "{'dataset': 'bank', 'templates': {'fc3d4b70-f59e-4fbb-9582-7e118946162c': None}}\n",
      "An unexpected error occurred: 'NoneType' object is not subscriptable\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Example 3 (Invalid File) ---\n",
      "Error: File not found at non_existent_file.yaml\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template_content = \"\"\"\n",
    "dataset: bank\n",
    "templates:\n",
    "  fc3d4b70-f59e-4fbb-9582-7e118946162c: !Template\n",
    "    name: bank\n",
    "    id: fc3d4b70-f59e-4fbb-9582-7e118946162c\n",
    "    reference: ''\n",
    "    answer_choices: 'No ||| Yes'\n",
    "    jinja: '{{note}}\n",
    "      \n",
    "      \n",
    "      Does this client subscribe to a term deposit? Yes or no?\n",
    "      Answer: \n",
    "      |||\n",
    "      {{ answer_choices[label] }}'\n",
    "    metadata: !TemplateMetadata\n",
    "      choices_in_prompt: true\n",
    "      metrics: ['accuracy']\n",
    "      original_task: true\n",
    "\"\"\"\n",
    "template_file_name = \"templates_bank.yaml\"\n",
    "with open(template_file_name, \"w\") as f:\n",
    "    f.write(template_content)\n",
    "\n",
    "# Example 1: Label 'Yes'\n",
    "sample_note_1 = \"Client has a good credit score and recently opened a savings account.\"\n",
    "sample_label_1 = 1 # Corresponds to 'Yes'\n",
    "print(\"--- Example 1 (Label 'Yes') ---\")\n",
    "result_1 = load_and_apply_template(template_file_name, sample_note_1, sample_label_1)\n",
    "print(result_1)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Label 'No'\n",
    "sample_note_2 = \"Client has a history of missed payments and high debt.\"\n",
    "sample_label_2 = 0 # Corresponds to 'No'\n",
    "print(\"--- Example 2 (Label 'No') ---\")\n",
    "result_2 = load_and_apply_template(template_file_name, sample_note_2, sample_label_2)\n",
    "print(result_2)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 3: Invalid file path\n",
    "print(\"--- Example 3 (Invalid File) ---\")\n",
    "result_3 = load_and_apply_template(\"non_existent_file.yaml\", \"Some note\", 0)\n",
    "print(result_3)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Clean up the dummy file\n",
    "# os.remove(template_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transhyper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
